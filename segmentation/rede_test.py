import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import traceback
import sys
from torch_geometric.data import DataLoader, Data
from torch_geometric.nn import GATConv, GINConv, global_mean_pool
from sklearn.model_selection import GroupShuffleSplit, train_test_split
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
from tqdm import tqdm
from torch.utils.data import Subset
import torch.nn as nn
import torch.nn.functional as F
from matplotlib.lines import Line2D
import pandas as pd
import json

class ECGGraph(Data):
    def __init__(self, x, edge_index, y, record_id, lead):
        super(ECGGraph, self).__init__(x=x, edge_index=edge_index, y=y)
        self.record_id = record_id
        self.lead = lead
    
    def __repr__(self):
        return f"ECGGraph(record={self.record_id}, lead={self.lead}, nodes={self.num_nodes}, edges={self.num_edges})"

# ================================================
# 1. Dataset: Loading and class definitions
# ================================================

# Path to the ECG dataset generated by the previous script
ecg_dataset_path = "ecg_visibility_graph_dataset.pt"

# Mapping for class names
CLASS_NAMES = ['P wave', 'QRS complex', 'T wave', 'unlabeled']

print("Loading dataset:")
all_graphs = []

try:
    # Load the dataset generated by the previous script with safety measures
    print(f"Trying to load dataset from path: {ecg_dataset_path}")
    if os.path.exists(ecg_dataset_path):
        # Define ECGGraph as a safe global object for deserialization
        import torch.serialization
        torch.serialization.add_safe_globals({'ECGGraph': ECGGraph})
        all_graphs = torch.load(ecg_dataset_path, weights_only=False)
        print(f"Dataset loaded successfully: {len(all_graphs)} graphs")
    else:
        print(f"File {ecg_dataset_path} not found.")
except Exception as e:
    print(f"Error loading datasets: {e}")
    traceback.print_exc()  # Print full stack trace for debugging

# ================================================
# 2. Split into train/validation maintaining class distribution
# ================================================

if len(all_graphs) == 0:
    print("No data loaded. Unable to continue training.")
    sys.exit(1)

def extract_graph_labels(graph):
    """Extract label distribution from a graph to help with stratification"""
    labels = graph.y.numpy()
    label_counts = {i: np.sum(labels == i) for i in range(4)}
    dominant_label = max(label_counts, key=label_counts.get)
    return dominant_label

graph_labels = [extract_graph_labels(graph) for graph in all_graphs]

train_indices, val_indices = train_test_split(
    range(len(all_graphs)), 
    test_size=0.2, 
    random_state=42,
    stratify=graph_labels
)

train_data = Subset(all_graphs, train_indices)
val_data = Subset(all_graphs, val_indices)

print(f"\nGraphs in TRAIN: {len(train_data)} (80%)")
print(f"Graphs in VAL: {len(val_data)} (20%)")

# ================================================
# 3. Class distribution in datasets
# ================================================

def compute_class_distribution(dataset):
    label_counts = {0: 0, 1: 0, 2: 0, 3: 0}  # P, QRS, T, unlabeled
    total_nodes = 0
    
    for i in range(len(dataset)):
        try:
            sample = dataset[i]
            for label in range(4):
                label_counts[label] += torch.sum(sample.y == label).item()
            total_nodes += len(sample.y)
        except Exception as e:
            print(f"Error processing sample {i}: {e}")
            continue
    
    return label_counts, total_nodes

train_counts, train_total = compute_class_distribution(train_data)
val_counts, val_total = compute_class_distribution(val_data)

print("\nClass distribution:")
for name, counts, total in [
    ("TRAIN", train_counts, train_total),
    ("VALIDATION", val_counts, val_total)
]:
    print(f"\n{name} - Total nodes: {total}")
    for label, count in counts.items():
        print(f"  {CLASS_NAMES[label]}: {count} ({count/total:.2f})")

# ================================================
# 4. Preparing DataLoaders
# ================================================

batch_size = 8 # Adjust according to your GPU
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)

# ================================================
# 5. Definition of Enhanced GAT+GIN model for node classification in ECG
# ================================================

class EnhancedECGGNN(nn.Module):
    def __init__(self, num_features, hidden_channels, num_classes, dropout=0.2):
        super(EnhancedECGGNN, self).__init__()
        
        # Feature extraction layers - separate paths for normal and inverted graph features
        # Modificação: A quantidade de features agora é diferente para o grafo normal e invertido
        # Grafo normal tem 6 features: amplitude, derivada, coordenada x, grau, flag1, flag2
        # Grafo invertido tem 3 features: grau, flag1, flag2
        self.normal_features_lin = nn.Linear(6, hidden_channels // 2)  # Para features do grafo normal (0-5)
        self.inverted_features_lin = nn.Linear(3, hidden_channels // 2)  # Para features do grafo invertido (6-8)
        
        # Block 1: GAT followed by GIN
        self.gat_conv1 = GATConv(hidden_channels, hidden_channels, heads=2, concat=False)
        self.mlp1 = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels),
            nn.ReLU(),
            nn.Linear(hidden_channels, hidden_channels)
        )
        self.conv1 = GINConv(self.mlp1)
        self.ln1 = nn.LayerNorm(hidden_channels)
        
        # Block 2: New GAT and GIN layer
        self.gat_conv2 = GATConv(hidden_channels, hidden_channels, heads=2, concat=False)
        self.mlp2 = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels),
            nn.ReLU(),
            nn.Linear(hidden_channels, hidden_channels)
        )
        self.conv2 = GINConv(self.mlp2)
        self.ln2 = nn.LayerNorm(hidden_channels)
        
        # Block 3: Third GAT and GIN layer to improve model capacity
        self.gat_conv3 = GATConv(hidden_channels, hidden_channels, heads=2, concat=False)
        self.mlp3 = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels),
            nn.ReLU(),
            nn.Linear(hidden_channels, hidden_channels)
        )
        self.conv3 = GINConv(self.mlp3)
        self.ln3 = nn.LayerNorm(hidden_channels)
        
        # Block 4: Fourth layer specifically for the enhanced feature set
        self.gat_conv4 = GATConv(hidden_channels, hidden_channels, heads=2, concat=False)
        self.mlp4 = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels),
            nn.ReLU(),
            nn.Linear(hidden_channels, hidden_channels)
        )
        self.conv4 = GINConv(self.mlp4)
        self.ln4 = nn.LayerNorm(hidden_channels)
        
        # Jumping Knowledge: concatenation of outputs from all 4 blocks
        self.lin_jump = nn.Linear(hidden_channels * 4, hidden_channels)
        
        # Final layers for node classification
        self.lin_final1 = nn.Linear(hidden_channels, hidden_channels)
        self.lin_final2 = nn.Linear(hidden_channels, num_classes)
        
        self.dropout = dropout

    def forward(self, data):
        # Pré-processamento: descartar nós antes do primeiro ponto rotulado
        # e nós após o último ponto rotulado.
        # Considera-se que os nós com label 0, 1 ou 2 são os nós rotulados
        # e o label 3 corresponde a "unlabeled".
        num_nodes = data.x.size(0)
        labeled_mask = data.y != 3  # True para nós com label válido (P, QRS ou T)
        if labeled_mask.sum() > 0:
            indices = torch.nonzero(labeled_mask, as_tuple=False).view(-1)
            first_idx = indices[0].item()
            last_idx = indices[-1].item()
            if first_idx > 0 or last_idx < num_nodes - 1:
                keep_range = torch.arange(first_idx, last_idx + 1, device=data.x.device)
                data.x = data.x[keep_range]
                data.y = data.y[keep_range]
                # Atualiza o edge_index: mantém apenas arestas com ambos os nós dentro da faixa e ajusta os índices
                mask = ((data.edge_index[0] >= first_idx) & (data.edge_index[0] <= last_idx) &
                        (data.edge_index[1] >= first_idx) & (data.edge_index[1] <= last_idx))
                data.edge_index = data.edge_index[:, mask] - first_idx

        # Divisão das features em normal e invertido
        # Modificação: Agora o grafo normal tem 6 features (0-5) e o invertido tem 3 features (6-8)
        normal_features = data.x[:, :6]      # Features 0-5 do grafo normal
        inverted_features = data.x[:, 6:9]   # Features 6-8 do grafo invertido

        # Processamento individual das features
        normal_emb = F.relu(self.normal_features_lin(normal_features))
        inverted_emb = F.relu(self.inverted_features_lin(inverted_features))
        x = torch.cat([normal_emb, inverted_emb], dim=1)
        
        # Block 1: GAT -> GIN with residual connection
        x_gat1 = self.gat_conv1(x, data.edge_index)
        x_gat1 = F.relu(x_gat1)
        x1_sub = self.conv1(x_gat1, data.edge_index)
        x1 = x_gat1 + F.dropout(x1_sub, p=self.dropout, training=self.training)
        x1 = self.ln1(x1)
        
        # Block 2: GAT -> GIN with residual connection
        x_gat2 = self.gat_conv2(x1, data.edge_index)
        x_gat2 = F.relu(x_gat2)
        x2_sub = self.conv2(x_gat2, data.edge_index)
        x2 = x_gat2 + F.dropout(x2_sub, p=self.dropout, training=self.training)
        x2 = self.ln2(x2)
        
        # Block 3: GAT -> GIN with residual connection
        x_gat3 = self.gat_conv3(x2, data.edge_index)
        x_gat3 = F.relu(x_gat3)
        x3_sub = self.conv3(x_gat3, data.edge_index)
        x3 = x_gat3 + F.dropout(x3_sub, p=self.dropout, training=self.training)
        x3 = self.ln3(x3)
        
        # Block 4: GAT -> GIN with residual connection
        x_gat4 = self.gat_conv4(x3, data.edge_index)
        x_gat4 = F.relu(x_gat4)
        x4_sub = self.conv4(x_gat4, data.edge_index)
        x4 = x_gat4 + F.dropout(x4_sub, p=self.dropout, training=self.training)
        x4 = self.ln4(x4)
        
        # Jumping Knowledge: concatenação dos recursos de todos os 4 blocos
        x_cat = torch.cat([x1, x2, x3, x4], dim=1)
        x_cat = F.relu(self.lin_jump(x_cat))
        x_cat = F.dropout(x_cat, p=self.dropout, training=self.training)
        
        # Camadas finais para classificação de cada nó
        x_cat = F.relu(self.lin_final1(x_cat))
        x_cat = F.dropout(x_cat, p=self.dropout, training=self.training)
        out = self.lin_final2(x_cat)
        
        return out

# ================================================
# 6. Instantiate model, criterion, and optimizer
# ================================================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

try:
    sample_graph = all_graphs[0]
    num_features = sample_graph.x.size(1)
    print(f"Automatically detected {num_features} features per node")
except Exception as e:
    print(f"Error accessing graph features: {e}")
    print("Using default value of 9 features")
    num_features = 9  # Atualizado para 9 features (6 normal + 3 invertido)

hidden_dim = 64
num_classes = 4  # P, QRS, T, unlabeled
dropout = 0.3

print(f"Number of node features: {num_features}")
print(f"Hidden dimension: {hidden_dim}")
print(f"Number of classes: {num_classes}")


model = EnhancedECGGNN(num_features, hidden_dim, num_classes, dropout=dropout).to(device)

def calculate_class_weights(train_counts):
    total = sum(train_counts.values())
    weights = [total / (train_counts[i] * 4 + 1e-6) for i in range(4)]
    return torch.tensor(weights, dtype=torch.float32).to(device)

class_weights = calculate_class_weights(train_counts)
print(f"Class weights: {class_weights}")

criterion = nn.CrossEntropyLoss(weight=class_weights)
learning_rate = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)

timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
results_dir = f"results/rede_test-{timestamp}/"
os.makedirs(results_dir, exist_ok=True)
confusion_matrix_file = f"{results_dir}/confusion_matrix.csv"

hyperparams = {
    "model": "EnhancedECGGNN",
    "num_features": num_features,
    "hidden_dim": hidden_dim,
    "batch_size": batch_size,
    "learning_rate": learning_rate,
    "dropout": dropout,
    "class_weights": class_weights.cpu().tolist(),
    "dataset": ecg_dataset_path
}

with open(f"{results_dir}/hyperparameters.json", "w") as f:
    json.dump(hyperparams, f, indent=4)

print(f"Results will be saved in: {results_dir}")

# ================================================
# 7. Training and evaluation functions
# ================================================

def train_epoch(loader):
    model.train()
    total_loss = 0
    total_correct = 0
    total_nodes = 0
    
    for data in tqdm(loader, desc="Training", leave=False):
        data = data.to(device)
        optimizer.zero_grad()
        
        out = model(data)
        loss = criterion(out, data.y)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * data.num_nodes
        pred = out.argmax(dim=1)
        total_correct += (pred == data.y).sum().item()
        total_nodes += data.num_nodes
    
    return total_loss / total_nodes, total_correct / total_nodes

@torch.no_grad()
def evaluate(loader):
    model.eval()
    total_loss = 0
    total_correct = 0
    total_nodes = 0
    
    all_preds = []
    all_labels = []
    
    for data in tqdm(loader, desc="Evaluating", leave=False):
        data = data.to(device)
        out = model(data)
        loss = criterion(out, data.y)
        
        total_loss += loss.item() * data.num_nodes
        pred = out.argmax(dim=1)
        total_correct += (pred == data.y).sum().item()
        total_nodes += data.num_nodes
        
        all_preds.append(pred.cpu())
        all_labels.append(data.y.cpu())
    
    all_preds = torch.cat(all_preds, dim=0).numpy()
    all_labels = torch.cat(all_labels, dim=0).numpy()
    
    f1_per_class = f1_score(all_labels, all_preds, average=None, zero_division=0)
    precision_per_class = precision_score(all_labels, all_preds, average=None, zero_division=0)
    recall_per_class = recall_score(all_labels, all_preds, average=None, zero_division=0)
    
    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)
    precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)
    recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)
    
    conf_matrix = confusion_matrix(all_labels, all_preds, labels=range(num_classes))
    
    avg_loss = total_loss / total_nodes
    accuracy = total_correct / total_nodes
    
    result = {
        'loss': avg_loss,
        'acc': accuracy,
        'f1_macro': f1_macro,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_per_class': f1_per_class,
        'precision_per_class': precision_per_class,
        'recall_per_class': recall_per_class,
        'conf_matrix': conf_matrix
    }
    
    return result

def save_confusion_matrix(epoch, conf_matrix, phase):
    conf_list = []
    for true_class in range(num_classes):
        row = [epoch, phase, CLASS_NAMES[true_class]] + conf_matrix[true_class].tolist()
        conf_list.append(row)
    
    column_names = ["Epoch", "Phase", "True_Label"] + [f"Pred_{name}" for name in CLASS_NAMES]
    conf_df = pd.DataFrame(conf_list, columns=column_names)
    
    if not os.path.exists(confusion_matrix_file):
        conf_df.to_csv(confusion_matrix_file, index=False)
    else:
        conf_df.to_csv(confusion_matrix_file, mode='a', header=False, index=False)

def plot_metric(metric_name, train_values, val_values, epoch, results_dir, ylabel):
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, epoch+1), train_values, label="Train", color="blue")
    plt.plot(range(1, epoch+1), val_values, label="Validation", color="orange")
    
    plt.xlabel("Epoch")
    plt.ylabel(ylabel)
    plt.title(f"{metric_name} Curve (Train vs Validation)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(f"{results_dir}/{metric_name.lower().replace(' ', '_')}_curve.png")
    plt.close()

# ================================================
# 8. Training loop
# ================================================

num_epochs = 60
best_val_f1 = 0.0

train_losses = []
train_accs = []
train_f1s = []
val_losses = []
val_accs = []
val_f1s = []

val_metrics = evaluate(val_loader)
print(f"Start | Val F1: {val_metrics['f1_macro']:.4f}")

for epoch in range(1, num_epochs+1):
    train_loss, train_acc = train_epoch(train_loader)
    train_metrics = evaluate(train_loader)
    
    val_metrics = evaluate(val_loader)
    
    train_losses.append(train_loss)
    train_accs.append(train_acc)
    train_f1s.append(train_metrics['f1_macro'])
    
    val_losses.append(val_metrics['loss'])
    val_accs.append(val_metrics['acc'])
    val_f1s.append(val_metrics['f1_macro'])
    
    scheduler.step(val_metrics['f1_macro'])
    
    if val_metrics['f1_macro'] > best_val_f1:
        best_val_f1 = val_metrics['f1_macro']
        torch.save(model.state_dict(), f"{results_dir}/best_model.pth")
        print(f"Epoch {epoch:02d} | New best model saved! F1: {best_val_f1:.4f}")
    
    print(f"Epoch {epoch:02d} | Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}, F1={train_metrics['f1_macro']:.4f} | "
          f"Val: Loss={val_metrics['loss']:.4f}, Acc={val_metrics['acc']:.4f}, F1={val_metrics['f1_macro']:.4f}")
    
    print("F1 by class (validation):")
    for i, f1 in enumerate(val_metrics['f1_per_class']):
        print(f"  {CLASS_NAMES[i]}: {f1:.4f}")
    
    save_confusion_matrix(epoch, val_metrics['conf_matrix'], "val")
    plot_metric("Loss", train_losses, val_losses, epoch, results_dir, "Loss")
    plot_metric("Accuracy", train_accs, val_accs, epoch, results_dir, "Accuracy")
    plot_metric("F1 Macro", train_f1s, val_f1s, epoch, results_dir, "F1 Macro")

model.load_state_dict(torch.load(f"{results_dir}/best_model.pth"))
val_metrics = evaluate(val_loader)

print("\n===== FINAL RESULTS =====")
print(f"Validation Loss: {val_metrics['loss']:.4f}")
print(f"Validation Accuracy: {val_metrics['acc']:.4f}")
print(f"Validation F1 Macro: {val_metrics['f1_macro']:.4f}")
print(f"Validation Precision Macro: {val_metrics['precision_macro']:.4f}")
print(f"Validation Recall Macro: {val_metrics['recall_macro']:.4f}")

print("\nF1 by class:")
for i, f1 in enumerate(val_metrics['f1_per_class']):
    print(f"  {CLASS_NAMES[i]}: {f1:.4f}")

print("\nNormalized confusion matrix:")
row_sums = val_metrics['conf_matrix'].sum(axis=1, keepdims=True)
norm_conf_matrix = np.divide(val_metrics['conf_matrix'], row_sums, where=row_sums!=0)
for i, row in enumerate(norm_conf_matrix):
    print(f"  {CLASS_NAMES[i]}: {row}")

final_results = {
    "val_loss": val_metrics['loss'],
    "val_accuracy": val_metrics['acc'],
    "val_f1_macro": val_metrics['f1_macro'],
    "val_precision_macro": val_metrics['precision_macro'],
    "val_recall_macro": val_metrics['recall_macro'],
    "val_f1_per_class": val_metrics['f1_per_class'].tolist(),
    "val_precision_per_class": val_metrics['precision_per_class'].tolist(),
    "val_recall_per_class": val_metrics['recall_per_class'].tolist(),
    "confusion_matrix": val_metrics['conf_matrix'].tolist(),
    "training_history": {
        "train_losses": train_losses,
        "train_accs": train_accs,
        "train_f1s": train_f1s,
        "val_losses": val_losses,
        "val_accs": val_accs,
        "val_f1s": val_f1s
    }
}

with open(f"{results_dir}/final_results.json", "w") as f:
    json.dump(final_results, f, indent=4)

print(f"\nResults saved in: {results_dir}/final_results.json")

# ================================================
# 10. Visualize a prediction example and compare durations
# ================================================

def visualize_prediction(graph, predictions, output_filename):
    plt.figure(figsize=(15, 14))
    
    # Modificado para refletir a nova estrutura de features
    ecg_signal = graph.x[:, 0].numpy()  # Amplitude do sinal original (índice 0)
    ecg_deriv = graph.x[:, 1].numpy()   # Primeira derivada (índice 1)
    ecg_x_coord = graph.x[:, 2].numpy() # Nova feature: coordenada x (índice 2)
    ecg_degrees = graph.x[:, 3].numpy() # Grau do nó no grafo normal (índice 3)
    inverted_degrees = graph.x[:, 6].numpy() # Grau do nó no grafo invertido (índice 6)
    
    true_labels = graph.y.numpy()
    pred_labels = predictions
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    plt.subplot(511)  # Alterado para 5 subplots
    for i in range(len(ecg_signal)):
        plt.plot(i, ecg_signal[i], 'o', color=colors[true_labels[i]], markersize=2)
    
    plt.title(f'ECG Signal - Record {graph.record_id}, Lead {graph.lead} - True Labels')
    plt.ylabel('Amplitude')
    plt.grid(True)
    
    
    legend_elements = [
        Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[0], markersize=8, label='P wave'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[1], markersize=8, label='QRS complex'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[2], markersize=8, label='T wave'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[3], markersize=8, label='Unlabeled')
    ]
    plt.legend(handles=legend_elements, loc='upper right')
    
    plt.subplot(512)
    for i in range(len(ecg_signal)):
        plt.plot(i, ecg_signal[i], 'o', color=colors[pred_labels[i]], markersize=2)
    
    plt.title('ECG Signal - Model Predictions')
    plt.ylabel('Amplitude')
    plt.grid(True)
    plt.legend(handles=legend_elements, loc='upper right')
    
    plt.subplot(513)
    plt.plot(range(len(ecg_signal)), ecg_signal, 'o', color='lightgray', markersize=2)
    errors = np.where(true_labels != pred_labels)[0]
    for i in errors:
        plt.plot(i, ecg_signal[i], 'o', color='red', markersize=4)
    
    plt.title(f'Prediction Errors (Total: {len(errors)} out of {len(ecg_signal)} points)')
    plt.ylabel('Amplitude')
    plt.grid(True)
    
    plt.subplot(514)
    plt.plot(ecg_deriv, 'b-', linewidth=1, label='First Derivative')
    plt.plot(ecg_x_coord, 'g-', linewidth=1, alpha=0.5, label='X Coordinate')
    
    for i in errors:
        plt.axvline(x=i, color='red', alpha=0.1)
    
    plt.title('First Derivative and X Coordinates')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(515)  # Adicionado novo subplot para os graus
    plt.plot(ecg_degrees, 'r-', linewidth=1, label='Normal Graph Degree')
    plt.plot(inverted_degrees, 'm-', linewidth=1, alpha=0.5, label='Inverted Graph Degree')
    
    for i in errors:
        plt.axvline(x=i, color='red', alpha=0.1)
    
    plt.title('Node Degrees')
    plt.xlabel('Node Index')
    plt.ylabel('Degree')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(output_filename, dpi=300)
    plt.close()
    print(f"Visualization saved as '{output_filename}'")